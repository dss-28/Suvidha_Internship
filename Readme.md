Here‚Äôs your updated README for the **new repo**, with the **earlier repo referenced** and **challenges section shortened** for clarity:

```markdown
# üöÄ Progressive Transformer-Based Summarization

> **From Single-Document to Multi-Document Abstractive Summarization**
> Internship baseline extended into a system-level NLP project (personal extension)

---

This repository presents a **continuous progression of an abstractive summarization system**, evolving from **single-document summarization** to **multi-document and multi-source summarization**.
The project focuses on how **model architecture, attention mechanisms, and pipeline design** must adapt as task complexity increases.

The original work (BART & LongT5) serves as a **baseline foundation** ([Internship Repo](https://github.com/dss-28/Suvidha_Internship/blob/main/README.md)), while the current extension (PEGASUS & LED) explores **multi-document summarization at scale**.

---

## üß≠ Project Evolution

```

Single-Document (Baseline) ‚îÄ‚îÄ‚ñ∂ Multi-Document (Extension)
Short News / Long Docs        News Aggregation / Long Multi-Source
BART, LongT5                 PEGASUS, LED

```

This progression reflects **real-world NLP system development**, where increasing input size and document cardinality demand changes not only in models, but also in **pipeline strategy**.

---

## üß© Phase I ‚Äî Single-Document Summarization (Baseline)

> *Summary only ‚Äî full details available in the [Internship Repo](https://github.com/dss-28/Suvidha_Internship/blob/main/README.md)*

### Models & Scope

* **BART (facebook/bart-large-cnn)** ‚Äî short news articles (CNN/DailyMail)
* **LongT5 (google/long-t5-tglobal-base)** ‚Äî long-form documents (ArxivSum)
* **Setting:** Inference-only (pretrained models)

### Key Takeaways

* BART excels at concise, structured news summarization
* LongT5 handles long-context inputs using sparse + global attention
* Input length and attention design strongly affect summary quality

---

## üß© Phase II ‚Äî Multi-Document Summarization (System Extension)

This phase extends the pipeline to **multi-document and multi-source summarization**, where redundancy, ordering bias, and context length become dominant challenges.

---

## üîπ PEGASUS ‚Äî Multi-Document News Summarization

### Dataset

* **MultiNews**
* **Evaluation Subset:** 1,000 validation + 1,000 test samples
* Each sample contains **multiple related news articles** describing the same event

### Why PEGASUS?

* Pretrained with **Gap Sentence Generation**, effective at identifying salient information
* Strong performance on news-style summarization
* Limited input length ‚Üí requires **pipeline adaptation**

### Pipeline Design

* Hierarchical / fusion-based pipeline:
  1. Individual documents summarized or compressed
  2. Salient information fused into a compact representation
  3. Final abstractive summary generated

### Results (Indicative)

| Metric  | Validation | Test |
| ------- | ---------- | ---- |
| ROUGE-1 | 32.4       | 31.7 |
| ROUGE-2 | 10.8       | 10.2 |
| ROUGE-L | 19.6       | 19.1 |

> üìå *Values are placeholders.*

---

## üîπ LED ‚Äî Long Multi-Document Summarization

### Dataset

* **WikiSum**
* **Evaluation Subset:** 1,000 validation + 1,000 test samples
* Each sample aggregates content from **multiple documents and sources**

### Why LED?

* Longformer-based **sparse attention**
* Supports very long inputs (8k‚Äì16k+ tokens)
* Designed to handle **concatenated multi-document contexts directly**

### Pipeline Design

* Documents concatenated with separators
* Global attention applied to key tokens
* No hierarchical preprocessing required for baseline evaluation

### Results (Indicative)

| Metric  | Validation | Test |
| ------- | ---------- | ---- |
| ROUGE-1 | 26.1       | 25.6 |
| ROUGE-2 | 6.9        | 6.5  |
| ROUGE-L | 15.8       | 15.4 |

---

## ‚ö†Ô∏è Challenges

* Compute limits on free Colab GPUs persisted; same batching/chunking strategies reused
* Multi-document handling required hierarchical/fusion for PEGASUS and concatenation for LED

---

## üî¨ Comparative Insights

| Aspect            | PEGASUS                        | LED                         |
| ----------------- | ------------------------------ | --------------------------- |
| Input Length      | Limited (~1k tokens)           | Very long (8k‚Äì16k+)         |
| Pipeline Need     | Hierarchical / fusion required | Direct ingestion sufficient |
| Strength          | Salient news aggregation       | Long-context reasoning      |
| Engineering Focus | Pipeline design                | Attention configuration     |

---

## üß† Key Learnings

* Multi-document summarization is a **systems problem**
* Hierarchical pipelines essential when context length is limited
* Long-context models reduce preprocessing needs but introduce efficiency trade-offs
* ROUGE less reliable as abstraction and document count increase

---

## üîÆ Future Work

* Lightweight fine-tuning (LoRA / adapters) on MultiNews or WikiSum
* Hybrid extractive ‚Üí abstractive pipelines
* Factuality and coherence evaluation beyond ROUGE
* Hierarchical extensions for LED for efficiency and consistency

---

## üß∞ Tech Stack

* **Language:** Python
* **Frameworks:** PyTorch, Hugging Face Transformers
* **Datasets:** CNN/DailyMail, ArxivSum, MultiNews, WikiSum
* **Evaluation:** ROUGE, Precision-based metrics
* **Environment:** Google Colab (GPU)

---

## üë§ Author

**Darshan Shirsat**  
MTech AI & Data Science

---

‚≠ê *Personal extension of single-document summarization to multi-document systems. If you find it useful, consider giving it a star.*
```

This version is **self-contained**, references your old internship repo, and keeps the challenges short and clear.

If you want, I can also **suggest a perfect repo description + topics** ready to paste on GitHub for maximum recruiter visibility. Do you want me to do that?
